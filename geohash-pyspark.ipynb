{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c66a917",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T10:56:19.169504Z",
     "iopub.status.busy": "2023-05-16T10:56:19.168651Z",
     "iopub.status.idle": "2023-05-16T10:57:07.259150Z",
     "shell.execute_reply": "2023-05-16T10:57:07.257770Z"
    },
    "papermill": {
     "duration": 48.100436,
     "end_time": "2023-05-16T10:57:07.261746",
     "exception": false,
     "start_time": "2023-05-16T10:56:19.161310",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\r\n",
      "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\r\n",
      "Building wheels for collected packages: pyspark\r\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \bdone\r\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317146 sha256=50b2ff7fb903492d06338db595fc43c1ea428212bb015624e7f3ef9e3a170f90\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/7b/1b/4b/3363a1d04368e7ff0d408e57ff57966fcdf00583774e761327\r\n",
      "Successfully built pyspark\r\n",
      "Installing collected packages: pyspark\r\n",
      "Successfully installed pyspark-3.4.0\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "688d21fb",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-05-16T10:57:07.289226Z",
     "iopub.status.busy": "2023-05-16T10:57:07.288750Z",
     "iopub.status.idle": "2023-05-16T10:57:07.296030Z",
     "shell.execute_reply": "2023-05-16T10:57:07.294809Z"
    },
    "papermill": {
     "duration": 0.023726,
     "end_time": "2023-05-16T10:57:07.298296",
     "exception": false,
     "start_time": "2023-05-16T10:57:07.274570",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85981b39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T10:57:07.325493Z",
     "iopub.status.busy": "2023-05-16T10:57:07.325117Z",
     "iopub.status.idle": "2023-05-16T10:57:12.991273Z",
     "shell.execute_reply": "2023-05-16T10:57:12.989852Z"
    },
    "papermill": {
     "duration": 5.683973,
     "end_time": "2023-05-16T10:57:12.995217",
     "exception": false,
     "start_time": "2023-05-16T10:57:07.311244",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/05/16 10:57:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "# Spark conf object\n",
    "\n",
    "#conf = SparkConf().setAppName(\"Geohash\")\n",
    "\n",
    "# Create Spark context object\n",
    "\n",
    "#sc = SparkContext(conf=conf)\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Geohash\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "add3f2d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T10:57:13.028005Z",
     "iopub.status.busy": "2023-05-16T10:57:13.027439Z",
     "iopub.status.idle": "2023-05-16T10:57:13.034110Z",
     "shell.execute_reply": "2023-05-16T10:57:13.032833Z"
    },
    "papermill": {
     "duration": 0.023336,
     "end_time": "2023-05-16T10:57:13.036127",
     "exception": false,
     "start_time": "2023-05-16T10:57:13.012791",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat, lit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69c4ba89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T10:57:13.063967Z",
     "iopub.status.busy": "2023-05-16T10:57:13.062838Z",
     "iopub.status.idle": "2023-05-16T10:57:17.128448Z",
     "shell.execute_reply": "2023-05-16T10:57:17.127357Z"
    },
    "papermill": {
     "duration": 4.082137,
     "end_time": "2023-05-16T10:57:17.130947",
     "exception": false,
     "start_time": "2023-05-16T10:57:13.048810",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([(1,40.7128,-74.0060),(2,37.7749, -122.4194)],\n",
    "                          ['ID','Lat','Long'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b176ab7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T10:57:17.159944Z",
     "iopub.status.busy": "2023-05-16T10:57:17.158722Z",
     "iopub.status.idle": "2023-05-16T10:57:17.351720Z",
     "shell.execute_reply": "2023-05-16T10:57:17.350691Z"
    },
    "papermill": {
     "duration": 0.209591,
     "end_time": "2023-05-16T10:57:17.354272",
     "exception": false,
     "start_time": "2023-05-16T10:57:17.144681",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Conactenate lat, long column \n",
    "df = df.withColumn(\"Geom\",concat(lit(\"(\"),df.Lat,lit(\", \"),df.Long,lit(\")\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7532695b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T10:57:17.381542Z",
     "iopub.status.busy": "2023-05-16T10:57:17.381170Z",
     "iopub.status.idle": "2023-05-16T10:57:21.028126Z",
     "shell.execute_reply": "2023-05-16T10:57:21.026923Z"
    },
    "papermill": {
     "duration": 3.664132,
     "end_time": "2023-05-16T10:57:21.031434",
     "exception": false,
     "start_time": "2023-05-16T10:57:17.367302",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---------+--------------------+\n",
      "| ID|    Lat|     Long|                Geom|\n",
      "+---+-------+---------+--------------------+\n",
      "|  1|40.7128|  -74.006|  (40.7128, -74.006)|\n",
      "|  2|37.7749|-122.4194|(37.7749, -122.4194)|\n",
      "+---+-------+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83538cf9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T10:57:21.066515Z",
     "iopub.status.busy": "2023-05-16T10:57:21.065993Z",
     "iopub.status.idle": "2023-05-16T10:57:34.134480Z",
     "shell.execute_reply": "2023-05-16T10:57:34.133012Z"
    },
    "papermill": {
     "duration": 13.086534,
     "end_time": "2023-05-16T10:57:34.137156",
     "exception": false,
     "start_time": "2023-05-16T10:57:21.050622",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pygeohash\r\n",
      "  Downloading pygeohash-1.2.0.tar.gz (5.0 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25hBuilding wheels for collected packages: pygeohash\r\n",
      "  Building wheel for pygeohash (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Created wheel for pygeohash: filename=pygeohash-1.2.0-py2.py3-none-any.whl size=6168 sha256=6fbafdda7ed035f06cf9894c80b4c299bf2e3e31c7276e0e9b9c049bb308df33\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/28/ec/b6/beadf7295a623f528507691fb0d471b50d064ae9bbad420b8f\r\n",
      "Successfully built pygeohash\r\n",
      "Installing collected packages: pygeohash\r\n",
      "Successfully installed pygeohash-1.2.0\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pygeohash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e92315",
   "metadata": {
    "papermill": {
     "duration": 0.01339,
     "end_time": "2023-05-16T10:57:34.164317",
     "exception": false,
     "start_time": "2023-05-16T10:57:34.150927",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ENCODE GEOHASH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ad7f32a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T10:57:34.193617Z",
     "iopub.status.busy": "2023-05-16T10:57:34.193207Z",
     "iopub.status.idle": "2023-05-16T10:57:34.278712Z",
     "shell.execute_reply": "2023-05-16T10:57:34.277849Z"
    },
    "papermill": {
     "duration": 0.103568,
     "end_time": "2023-05-16T10:57:34.281393",
     "exception": false,
     "start_time": "2023-05-16T10:57:34.177825",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pygeohash as pgh\n",
    "import pyspark.sql.functions as F \n",
    "\n",
    "geohash_udf = F.udf(lambda x,y: pgh.encode(x,y,precision=7))\n",
    "df = df.select(\"ID\",\"Geom\",\"Lat\",'Long',geohash_udf('Lat','Long').alias('encoded_val7'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf014175",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T10:57:34.311461Z",
     "iopub.status.busy": "2023-05-16T10:57:34.310722Z",
     "iopub.status.idle": "2023-05-16T10:57:36.934111Z",
     "shell.execute_reply": "2023-05-16T10:57:36.932740Z"
    },
    "papermill": {
     "duration": 2.643059,
     "end_time": "2023-05-16T10:57:36.938749",
     "exception": false,
     "start_time": "2023-05-16T10:57:34.295690",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-------+---------+------------+\n",
      "| ID|                Geom|    Lat|     Long|encoded_val7|\n",
      "+---+--------------------+-------+---------+------------+\n",
      "|  1|  (40.7128, -74.006)|40.7128|  -74.006|     dr5regw|\n",
      "|  2|(37.7749, -122.4194)|37.7749|-122.4194|     9q8yyk8|\n",
      "+---+--------------------+-------+---------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed15b7f",
   "metadata": {
    "papermill": {
     "duration": 0.022993,
     "end_time": "2023-05-16T10:57:36.984589",
     "exception": false,
     "start_time": "2023-05-16T10:57:36.961596",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# DECODE GEOHASH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e07f8e06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T10:57:37.027631Z",
     "iopub.status.busy": "2023-05-16T10:57:37.027144Z",
     "iopub.status.idle": "2023-05-16T10:57:37.032991Z",
     "shell.execute_reply": "2023-05-16T10:57:37.031853Z"
    },
    "papermill": {
     "duration": 0.032253,
     "end_time": "2023-05-16T10:57:37.035667",
     "exception": false,
     "start_time": "2023-05-16T10:57:37.003414",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType,ArrayType\n",
    "import pygeohash as pgh "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67a4f447",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T10:57:37.077681Z",
     "iopub.status.busy": "2023-05-16T10:57:37.077185Z",
     "iopub.status.idle": "2023-05-16T10:57:37.083626Z",
     "shell.execute_reply": "2023-05-16T10:57:37.082806Z"
    },
    "papermill": {
     "duration": 0.028888,
     "end_time": "2023-05-16T10:57:37.085838",
     "exception": false,
     "start_time": "2023-05-16T10:57:37.056950",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "udf2 = F.udf(lambda x: pgh.decode(x),ArrayType(FloatType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "834b95c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T10:57:37.115452Z",
     "iopub.status.busy": "2023-05-16T10:57:37.114653Z",
     "iopub.status.idle": "2023-05-16T10:57:37.177663Z",
     "shell.execute_reply": "2023-05-16T10:57:37.176768Z"
    },
    "papermill": {
     "duration": 0.081032,
     "end_time": "2023-05-16T10:57:37.180532",
     "exception": false,
     "start_time": "2023-05-16T10:57:37.099500",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_new = df.select('ID','Geom','Lat','Long','encoded_val7',udf2('encoded_val7').alias('decodedVal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f6f76f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T10:57:37.211107Z",
     "iopub.status.busy": "2023-05-16T10:57:37.210711Z",
     "iopub.status.idle": "2023-05-16T10:57:38.319731Z",
     "shell.execute_reply": "2023-05-16T10:57:38.318470Z"
    },
    "papermill": {
     "duration": 1.128141,
     "end_time": "2023-05-16T10:57:38.323176",
     "exception": false,
     "start_time": "2023-05-16T10:57:37.195035",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-------+---------+------------+----------------+\n",
      "| ID|                Geom|    Lat|     Long|encoded_val7|      decodedVal|\n",
      "+---+--------------------+-------+---------+------------+----------------+\n",
      "|  1|  (40.7128, -74.006)|40.7128|  -74.006|     dr5regw| [40.71, -74.01]|\n",
      "|  2|(37.7749, -122.4194)|37.7749|-122.4194|     9q8yyk8|[37.77, -122.42]|\n",
      "+---+--------------------+-------+---------+------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4365986a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T10:57:38.372618Z",
     "iopub.status.busy": "2023-05-16T10:57:38.372059Z",
     "iopub.status.idle": "2023-05-16T10:57:38.411897Z",
     "shell.execute_reply": "2023-05-16T10:57:38.410853Z"
    },
    "papermill": {
     "duration": 0.069563,
     "end_time": "2023-05-16T10:57:38.414528",
     "exception": false,
     "start_time": "2023-05-16T10:57:38.344965",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ID', 'Geom', 'Lat', 'Long', 'encoded_val7', 'decodedVal']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9aec496",
   "metadata": {
    "papermill": {
     "duration": 0.015324,
     "end_time": "2023-05-16T10:57:38.457895",
     "exception": false,
     "start_time": "2023-05-16T10:57:38.442571",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- As the decoded string is a “List”, it needs to be split into separate Lat/Long columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cae1a31",
   "metadata": {
    "papermill": {
     "duration": 0.014548,
     "end_time": "2023-05-16T10:57:38.488739",
     "exception": false,
     "start_time": "2023-05-16T10:57:38.474191",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d96ec5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T10:57:38.521008Z",
     "iopub.status.busy": "2023-05-16T10:57:38.519927Z",
     "iopub.status.idle": "2023-05-16T10:57:38.583108Z",
     "shell.execute_reply": "2023-05-16T10:57:38.581983Z"
    },
    "papermill": {
     "duration": 0.08295,
     "end_time": "2023-05-16T10:57:38.586380",
     "exception": false,
     "start_time": "2023-05-16T10:57:38.503430",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import col\n",
    "# split the decoded value into latitude and longitude columns\n",
    "df_new = df_new.withColumn(\"latitude\", col(\"decodedVal\")[0].cast(\"float\"))\n",
    "df_new = df_new.withColumn(\"longitude\", col(\"decodedVal\")[1].cast(\"float\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ce3fe2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T10:57:38.618531Z",
     "iopub.status.busy": "2023-05-16T10:57:38.617921Z",
     "iopub.status.idle": "2023-05-16T10:57:39.709779Z",
     "shell.execute_reply": "2023-05-16T10:57:39.708150Z"
    },
    "papermill": {
     "duration": 1.111451,
     "end_time": "2023-05-16T10:57:39.713160",
     "exception": false,
     "start_time": "2023-05-16T10:57:38.601709",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-------+---------+------------+----------------+--------+---------+\n",
      "| ID|                Geom|    Lat|     Long|encoded_val7|      decodedVal|latitude|longitude|\n",
      "+---+--------------------+-------+---------+------------+----------------+--------+---------+\n",
      "|  1|  (40.7128, -74.006)|40.7128|  -74.006|     dr5regw| [40.71, -74.01]|   40.71|   -74.01|\n",
      "|  2|(37.7749, -122.4194)|37.7749|-122.4194|     9q8yyk8|[37.77, -122.42]|   37.77|  -122.42|\n",
      "+---+--------------------+-------+---------+------------+----------------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b594e14",
   "metadata": {
    "papermill": {
     "duration": 0.014287,
     "end_time": "2023-05-16T10:57:39.743535",
     "exception": false,
     "start_time": "2023-05-16T10:57:39.729248",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 94.419562,
   "end_time": "2023-05-16T10:57:42.383079",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-05-16T10:56:07.963517",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
